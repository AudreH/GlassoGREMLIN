V <- dnorm(seq(-4,4,length=100),0,1)
plot(V,type='l')
K <- matrix(0,n,n)
for (i in 1:n){
for (j in ((i+1):n)){
K[i,j] = sign(V[j]-V[i])
}
}
n <- 100
V <- dnorm(seq(-4,4,length=100),0,1)
plot(V,type='l')
K <- matrix(0,n,n)
for (i in 1:n){
for (j in ((i+1):n)){
K[i,j] = sign(V[j]-V[i])
}
}
n <- 100
V <- dnorm(seq(-4,4,length=n),0,1)
plot(V,type='l')
K <- matrix(0,n,n)
for (i in 1:(n-1)){
for (j in ((i+1):n)){
K[i,j] = sign(V[j]-V[i])
}
}
K
sum(sum(K))
sum(sum(K!= 0))
100*100/2
n*(n-1)/2
V==0
x <- seq(-4,0,length= n/2),seq(0,4,length= n/2)
x <- c(seq(-4,0,length= n/2),seq(0,4,length= n/2))
x
x <- unique(c(seq(-4,0,length= n/2),seq(0,4,length= n/2)))
p <- 100
x <- unique(c(seq(-4,0,length= p/2),seq(0,4,length= p/2)))
n <- length(x)
V <- dnorm(x,0,1)
plot(V,type='l')
K <- matrix(0,n,n)
p <- 100
x <- unique(c(seq(-4,0,length= p/2),seq(0,4,length= p/2)))
n <- length(x)
V <- dnorm(x,0,1)
plot(V,type='l')
K <- 0
for (i in 1:(n - 1)){
for (j in ((i + 1):n)){
K  <- K +  sign(V[j] - V[i])
}
}
K
K * 2/(n * (n - 1))
library(keras)
##########################################################################
## ----iris----------------------------------------------------------------
###########################################################################"
data(iris)
library(tidyverse)
x_iris <-
iris %>%
select(-Species) %>%
as.matrix %>%
scale
library(keras)
y_iris <- to_categorical(as.integer(iris$Species)-1)
set.seed(0)
ntest <- 15 # number of test samples in each class
test.index <-
tibble(row_number =1:nrow(iris),Species = iris$Species)  %>% group_by(Species) %>% sample_n(ntest) %>% pull(row_number)
train.index <- (1:nrow(iris))[-test.index]
x_iris_train <- x_iris[train.index,]
y_iris_train <- y_iris[train.index,]
x_iris_test <- x_iris[test.index,]
y_iris_test <- y_iris[test.index,]
## ---- eval=FALSE---------------------------------------------------------
model_iris <- keras_model_sequential()
model_iris %>%
layer_dense(units = 4, input_shape = 4) %>%
layer_dropout(rate = 0.1) %>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 3) %>%
layer_activation(activation = 'softmax')
model_iris %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
learning_history <- model_iris %>% fit(x_iris_train, y_iris_train, epochs = 200, validation_split=0.0)## loss_and_metrics <- model %>% evaluate(x_iris_test, y_iris_test)
model_autoencoder <- keras_model_sequential()
model_autoencoder %>%
layer_dense(units = 2, activation = 'linear',input_shape = ncol(x_iris),name = "inter_layer") %>%
layer_dense(units = 4, activation = 'linear')
model_autoencoder %>% compile(
loss = 'mse',
optimizer = 'adam',
metrics = 'mse'
)
model_autoencoder %>%
layer_dense(units = 2, activation = 'linear',input_shape = ncol(x_iris),name = "inter_layer") %>%
layer_dense(units = 4, activation = 'linear')
model_autoencoder %>% compile(
loss = 'mse',
optimizer = 'adam',
metrics = 'mse'
)
model_autoencoder <- keras_model_sequential()
model_autoencoder %>%
layer_dense(units = 2, activation = 'linear',input_shape = ncol(x_iris),name = "inter_layer") %>%
layer_dense(units = 4, activation = 'linear')
model_autoencoder %>% compile(
loss = 'mse',
optimizer = 'adam',
metrics = 'mse'
)
model_autoencoder %>% fit(
x_iris_train,
x_iris_train,
epochs = 100,
batch_size = 16,
shuffle  = TRUE,
validation_split = 0.1,
)
model_projection = keras_model(inputs = model_autoencoder$input, outputs = get_layer(model_autoencoder,"inter_layer")$output)
##
intermediate_output = predict(model_projection,x_iris_train)
## ---- eval = FALSE-------------------------------------------------------
library(FactoMineR)
res.pca <- PCA(x_iris_train, graph = FALSE)
par(mfrow=c(1,2))
plot(intermediate_output[,1],intermediate_output[,2],col = y_iris_train %*% (1:3))
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2], col = y_iris_train %*% (1:3))
##########################################################################
## ----iris----------------------------------------------------------------
###########################################################################"
library(keras)
data(iris)
library(tidyverse)
x_iris <-
iris %>%
select(-Species) %>%
as.matrix %>%
scale
library(keras)
y_iris <- to_categorical(as.integer(iris$Species)-1)
set.seed(0)
ntest <- 15 # number of test samples in each class
test.index <-
tibble(row_number =1:nrow(iris),Species = iris$Species)  %>% group_by(Species) %>% sample_n(ntest) %>% pull(row_number)
train.index <- (1:nrow(iris))[-test.index]
x_iris_train <- x_iris[train.index,]
y_iris_train <- y_iris[train.index,]
x_iris_test <- x_iris[test.index,]
y_iris_test <- y_iris[test.index,]
model_iris <- keras_model_sequential()
model_iris %>%
layer_dense(units = 4, input_shape = 4) %>%
layer_dropout(rate = 0.1) %>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 3) %>%
layer_activation(activation = 'softmax')
model_iris %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
learning_history <- model_iris %>% fit(x_iris_train, y_iris_train, epochs = 200, validation_split=0.0)## loss_and_metrics <- model %>% evaluate(x_iris_test, y_iris_test)
estimation <- apply(predict(model_iris,x_iris_test),1,which.max)
truth <- apply(y_iris_test,1,which.max)
table(estimation, truth)
model_autoencoder <- keras_model_sequential()
model_autoencoder %>% compile(
loss = 'mse',
optimizer = 'adam',
metrics = 'mse'
)
model_autoencoder %>% fit(
x_iris_train,
x_iris_train,
epochs = 100,
batch_size = 16,
shuffle  = TRUE,
validation_split = 0.1,
)
model_projection = keras_model(inputs = model_autoencoder$input, outputs = get_layer(model_autoencoder,"inter_layer")$output)
model_autoencoder <- keras_model_sequential()
model_autoencoder %>%
layer_dense(units = 2, activation = 'linear',input_shape = ncol(x_iris),name = "inter_layer") %>%
layer_dense(units = 4, activation = 'linear')
model_autoencoder %>% compile(
loss = 'mse',
optimizer = 'adam',
metrics = 'mse'
)
model_autoencoder %>% fit(
x_iris_train,
x_iris_train,
epochs = 100,
batch_size = 16,
shuffle  = TRUE,
validation_split = 0.1,
)
model_projection = keras_model(inputs = model_autoencoder$input, outputs = get_layer(model_autoencoder,"inter_layer")$output)
##
intermediate_output = predict(model_projection,x_iris_train)
intermediate_output
## ---- eval = FALSE-------------------------------------------------------
library(FactoMineR)
res.pca <- PCA(x_iris_train, graph = FALSE)
par(mfrow=c(1,2))
plot(intermediate_output[,1],intermediate_output[,2],col = y_iris_train %*% (1:3))
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2], col = y_iris_train %*% (1:3))
plot(intermediate_output[,1],intermediate_output[,2],col = y_iris_train %*% (1:3))
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2], col = y_iris_train %*% (1:3))
install.packages("pkgdown")
help(sign)
sign(0)
sign(2)
library(gmp)
Stirling1(100,10)
2^100
help("Stirling1")
lm()
ls()
nFG
vNQ
source("utils_laplace.R")
source("VEM_SBMLaplace.R")
Lambda <- matrix(c(1, 2, 3, 2, 1, 4, 3, 4, 1), 3, 3)
p <- 200
alpha <- c(1/2, 1/4, 1/4)
mySBM <- rSBMLaplace(p, Lambda, alpha)
cl <- V(mySBM)$membership
Omega <- mySBM %>% as_adj(attr = "weight")
diag(Omega)
# HOW to charge the diagonal
eta <- 1;
Omega[1,1] = rexp(1,eta)
for (i in 1:nrow(Omega)){
Omega[i,i] <- rexp(1,eta)   +
}
image(Omega[order(cl), order(cl)])
vBlocks <- 1:5
out <- lapply(vBlocks, function(q) VEM_SBM_laplace(Omega, q))
plot(vBlocks, sapply(out, function(o) o$vICL), type = "l", main = "vICL")
setwd("D:/WORK_ALL/RECHERCHE/TRAVAUX_RECHERCHE/Julien-Audrey-Florence/SBM_laplace_prior/code")
source("utils_laplace.R")
source("VEM_SBMLaplace.R")
Lambda <- matrix(c(1, 2, 3, 2, 1, 4, 3, 4, 1), 3, 3)
p <- 200
alpha <- c(1/2, 1/4, 1/4)
mySBM <- rSBMLaplace(p, Lambda, alpha)
cl <- V(mySBM)$membership
Omega <- mySBM %>% as_adj(attr = "weight")
diag(Omega)
# HOW to charge the diagonal
eta <- 1;
Omega[1,1] = rexp(1,eta)
for (i in 1:nrow(Omega)){
Omega[i,i] <- rexp(1,eta)   +
}
image(Omega[order(cl), order(cl)])
vBlocks <- 1:5
out <- lapply(vBlocks, function(q) VEM_SBM_laplace(Omega, q))
plot(vBlocks, sapply(out, function(o) o$vICL), type = "l", main = "vICL")
# HOW to charge the diagonal
eta <- 1;
Omega[1,1] = rexp(1,eta)
i=2
U <- rexp(1,eta)
Binf <-
Omegaii  = Omega[1:i,1:i]
Binf <-  0
Omegaii  = Omega[1:i,1:i]
Omegaii
for (j in 1:(i-1)){
Binf <- Binf + (-1)^{i+j}*det(Omegaii[-i,j])
}
Binf <- Binf + (-1)^(i+j)*det(Omegaii[-i,j])
i
U <- rexp(1,eta)
Binf <-  0
Omegaii  = Omega[1:i,1:i]
Omegaii
j = 1
det(Omegaii[-i,j]
)
det(as.numeric(Omegaii[-i,j]))
Omega
dim(Omegaii)
Omegaii
determinant(Omegaii[-i,j])
Omegaii[-i,-j]
Binf <- Binf + (-1)^(i+j)*Omegaii[i,j]*determinant(Omegaii[-i,-j]))
Binf <- Binf + (-1)^(i+j)*Omegaii[i,j]*determinant(Omegaii[-i,-j])
determinant(Omegaii[-i,-j])
det(Omegaii[-i,j]
)
Omegaii[-i,-j]
det(Omegaii[-i,-j])
determinant.matrix(Omegaii[-i,-j])
determinant.matrix(as.matrix(Omegaii[-i,-j],1,1))
determinant.matrix(as.matrix(Omegaii[-i,-j],1,1))$modulus
determinant.matrix(as.matrix(Omegaii[-i,-j],1,1),log=FALSE)$modulus
Omega <- as.matrix(Omega)
Omega[1:2,1:2]
U <- rexp(1,eta)
Binf <-  0
Omegaii  = Omega[1:i,1:i]
Omegaii
Binf + (-1)^(i + j)*Omegaii[i,j]*determinant(Omegaii[-i,-j])
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(Omegaii[-i,-j])
is.matrix(Omegaii)
Omegaii[-i,-j]
is.matrix(Omegaii[-i,-j])
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
U <- rexp(1,eta)
Binf <-  0
Omegaii  = as.matrix(Omega[1:i,1:i])
for (j in 1:(i-1)){
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
}
Binf
Omega[i,i] <- Binf / (-1)^{i+i}*det(as.matrix(Omegaii[-i,-i]))
Omega[i,i] <- Binf / ((-1)^{i+i}*det(as.matrix(Omegaii[-i,-i]))) + U
for (i in 2:nrow(Omega)){
U <- rexp(1,eta)
Binf <-  0
Omegaii  = as.matrix(Omega[1:i,1:i])
for (j in 1:(i-1)){
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
}
Omega[i,i] <- Binf / ((-1)^{i+i}*det(as.matrix(Omegaii[-i,-i]))) + U
}
Omega
diag(Omega)
dim(Omega)
i=200
U <- rexp(1,eta)
Binf <-  0
Omegaii  = as.matrix(Omega[1:i,1:i])
for (j in 1:(i-1)){
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
}
Binf
j=1
Binf <-  0
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
Binf
j=1
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
Omegaii  = as.matrix(Omega[1:i,1:i])
for (j in 1:(i-1)){
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
print(c(j,Binf))
}
Binf <-  0
Omegaii  = as.matrix(Omega[1:i,1:i])
for (j in 1:(i-1)){
Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
print(c(j,Binf))
}
U <- rexp(1,eta)
Binf <-  0
Omegaii  = as.matrix(Omega[1:i,1:i])
j
(-1)^(i + j)
Omegaii[i,j]
det(as.matrix(Omegaii[-i,-j]))
Omegaii[-i,-j]
det(as.matrix(Omegaii[-i,-j]),log=TRUE)
diag(Omega) = 0
Omega[i,i]  = rexp(1)+ sum(abs(Omega[i,]))
Omega[i,i]
eta <- 1;
diag(Omega) = 0
for (i in 2:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
eta <- 1;
diag(Omega) = 0
for (i in 2:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
eigen(Omega)$values
# HOW to charge the diagonal  : sol 1 : using the charging the diagonal
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
source("utils_laplace.R")
source("VEM_SBMLaplace.R")
Lambda <- matrix(c(1, 2, 3, 2, 1, 4, 3, 4, 1), 3, 3)
p <- 200
alpha <- c(1/2, 1/4, 1/4)
mySBM <- rSBMLaplace(p, Lambda, alpha)
cl <- V(mySBM)$membership
Omega <- mySBM %>% as_adj(attr = "weight")
diag(Omega)
image(Omega[order(cl), order(cl)])
# HOW to charge the diagonal  : sol 1 : using the principal minors...  numerical pob .
# eta <- 1;
# Omega[1,1] = rexp(1,eta)
# Omega <- as.matrix(Omega)
# for (i in 2:nrow(Omega)){
#       U <- rexp(1,eta)
#       Binf <-  0
#       Omegaii  = as.matrix(Omega[1:i,1:i])
#       for (j in 1:(i-1)){
#         Binf <- Binf + (-1)^(i + j)*Omegaii[i,j]*det(as.matrix(Omegaii[-i,-j]))
#         print(c(j,Binf))
#       }
#
#
#       Omega[i,i] <- Binf / ((-1)^{i+i}*det(as.matrix(Omegaii[-i,-i]))) + U
# }
#
# HOW to charge the diagonal  : sol 1 : using the charging the diagonal
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
vBlocks <- 1:5
out <- lapply(vBlocks, function(q) VEM_SBM_laplace(Omega, q))
plot(vBlocks, sapply(out, function(o) o$vICL), type = "l", main = "vICL")
source("utils_laplace.R")
source("VEM_SBMLaplace.R")
Lambda <- matrix(c(1, 2, 3, 2, 1, 4, 3, 4, 1), 3, 3)
p <- 200
alpha <- c(1/2, 1/4, 1/4)
mySBM <- rSBMLaplace(p, Lambda, alpha)
mySBM
Omega <- mySBM %>% as_adj(attr = "weight")
diag(Omega)
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
# HOW to charge the diagonal  : sol 2 : using the charge the diagonal  : very large values on the diagonal
eta <- 1;
diag(Omega) = 0
for (i in 1:nrow(Omega)){
eps <- rexp(1)
Omega[i,i]  = eps + sum(abs(Omega[i,]))
}
min(eigen(Omega)$values)
