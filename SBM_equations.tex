\documentclass[10pt]{article}

%\setlength{\columnsep}{0.5cm}
%\setlength\textwidth{\dimexpr (8.6cm)*2 + 0.5cm\relax}

\usepackage{ragged2e}

\usepackage{abstract}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{subcaption}
%\usepackage{arxiv}
\usepackage[draft]{todonotes}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{multirow}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage[left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
%\usepackage[paperwidth=210mm,
%            paperheight=297mm,
%            left=50pt,
%            top=50pt,
%            textwidth=345pt,
%            marginparsep=25pt,
%            marginparwidth=124pt,
%            textheight=692pt,
%            footskip=50pt]
%           {geometry}
%\graphicspath{{../Figures/}}

\definecolor{darkgreen}{rgb}{0, 0.5, 0} 
\definecolor{darkred}{rgb}{0.5, 0, 0} 

\title{SBM Laplace prior for concentration matrix}



\begin{document}
\newcommand*{\norm}[1]{\left\lVert{#1}\right\rVert}

\maketitle

\section{Model}\label{sec:model}

Data: $X$, $i\in \{ 0,...,n \}$ observations. $X \sim \mathcal{N}(0, \Sigma^{-1} = \Omega)$, $\Omega$ concentration matrix. Assumption: $Q$ hidden clusters in the data. For each observation $i$, $Z_{iq} = \mathds{1}_{i\in q}$, $ q\in \{ 1,...,Q \}$. $Z_{iq}$ = independant latent variables.

\begin{itemize}
	\item $\alpha_q = \mathbb{P}(i\in q)$ (Probability that observation i belongs to cluster q); 
	\item $\sum_q \alpha_q = 1 $;
	\item $ \underline{Z_i} \sim \mathcal{M}(1, \underline{\alpha})$ with $\underline{\alpha} = (\alpha_1, ..., \alpha_Q)$  and $\underline{Z_i} = (Z_{i1}, ..., Z_{iQ})$
\end{itemize}

Assumption: 

\[ \Omega_{ij}| \{ Z_{iq}Z_{jl} \} \sim Laplace(0, \lambda_{ql}), ~~ (i,j) \in \{ 1,...,n \}^2, (q,l)\in \{ 1,...,Q \}^2 \]

Laplace distribution: 

\[ \forall x \in \mathbb{R},~~ 
f_{ql}(x) = \frac{1}{2\lambda_{ql}} \exp{-\frac{|x|}{\lambda_{ql}}} ~~ \text{if} ~~ q \neq l 
~~\text{and} ~~
f_0{x} = \frac{1}{2\lambda_{0}} \exp{-\frac{|x|}{\lambda_{0}}} ~~ \text{otherwise}\]

where $\lambda_{ql}, \lambda_0 > 0$ are scaling parameters and $\lambda_{ql} = \lambda_{lq}$. $\Lambda$ matrix of parameters $\lambda_{ql}$, $(q,l) \in \{1,...,Q\}^2$.



\section{Complete likelihood}

Assumptions reminder:

\[ X|\Omega \sim \mathcal{N}(O, \Omega^{-1}),~~ \Omega|Z \sim Laplace(0, \Lambda),~~ Z \sim \mathcal{M}(1,\underline{\alpha}) \]

Complete likelihood decomposition:
\begin{align*}
L_c(X, \Omega, Z) &= \mathbb{P}(X,\Omega, Z) \\
           & =  \mathbb{P}(X|\{\Omega, Z\})\mathbb{P}(\Omega|Z)\mathbb{P}(Z) ~~ \text{conditional probabilities definition/formula} \\
           & =  \mathbb{P}(X|\Omega)\mathbb{P}(\Omega|Z)\mathbb{P}(Z) \\
\end{align*}

$\mathbb{P}(X| \{ \Omega, Z \}) =  \mathbb{P}(X|\Omega)$, known distribution (knowing $Z$ is equivalent to know $\Omega ?$). 

Complete log-likelihood formula: (when $i \neq j$, otherwise replace $\lambda_{ql}$ with $\lambda_0$)
\begin{align*}
\log L_c(X, \Omega, Z) &= \log \mathbb{P}(X,\Omega, Z) \\
           & =  \underbrace{\log \mathbb{P}(X|\Omega)}_{(1)}+ \underbrace{\log \mathbb{P}(\Omega|Z)}_{(2)} + \underbrace{\log \mathbb{P}(Z)}_{(3)} \\
           & = \frac{n}{2} \left( \log(|\Omega|) - tr(S\Omega)  - p\log(2\pi) \right) +  \sum_{q,l,i,j , i \neq j} Z_{iq}Z_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}} \right) \\
           & + \sum_{i = 1}^n \sum_{q = 1}^Q Z_{iq} \log \alpha_q
\end{align*}

Penalty term (lasso approach) in the article: $\sum_{q,l,i,j , i \neq j} Z_{iq}Z_{jl}\frac{|\Omega_{ij}|}{\lambda_{ql}}$. In the paper: notation $\norm{\rho_Z(\Omega)}_{l_1}$.

\paragraph{(1):}  $X|\Omega \sim \mathcal{N}(0, \Omega^{-1})$

Notation: $|\Omega| = det(\Omega) = 1/|\Omega^{-1}|$
%\begin{align*}
%\log \mathbb{P}(X|\Omega) &= \log \left( \frac{1}{(2\pi)^{\frac{n}{2}} |\Omega|}  \exp(x^{T} \Omega^{-1} x) \right) \\
%		& = -\frac{n}{2} \left( \log(2\pi) - \log(|\Omega^{-1}| \right) + x^{T} \Omega^{-1} x \\ 
%		& = -\frac{n}{2} \left( \log(2\pi) + \log(|\Omega|) \right) + x^{T} \Omega^{-1} x \\ 
%\end{align*}

\begin{align*}
\log \mathbb{P}(X|\Omega) &= \sum_{i=1}^n \log \left( \frac{1}{(2\pi)^{\frac{p}{2}} |\Omega^{-1}|}  \exp(\frac{1}{2} x_i^{T} \Omega x_i) \right) \\
		& = -\frac{n}{2} \left( p\log(2\pi) + \log(|\Omega^{-1}| \right) - \frac{1}{2}tr(X^TX\Omega) \\ 
		& = -\frac{n}{2} \left( p\log(2\pi) - \log(|\Omega|) \right) - \frac{n}{2}tr(S\Omega)~ \text{En notant } S = \frac{1}{n}X^TX \\ 
		& = \frac{n}{2} \left( - p\log(2\pi) + \log(|\Omega|) \right) - \frac{n}{2}tr(S\Omega) \\ 
		& = \frac{n}{2} \left( \log(|\Omega|) - tr(S\Omega)  - p\log(2\pi) \right) \\
\end{align*}


\paragraph{(2):}  $\Omega|Z \sim Laplace(0, \Lambda)$


\textcolor{red}{Sophie : ici il faut changer tous les $n$ en $p$ = taille de $\Omega$. }
If $i\neq j$:
\begin{align*}
\log \mathbb{P}(\Omega|Z) &= \log \prod_{i,j,q,l} \mathbb{P}(\Omega_{ij}|\{Z_{iq}Z_{jl}\})^{Z_{iq}Z_{jl}}~ \text{Astuce termes puissance 0 ne participent pas au produit} \\ 
	& = \sum_{\substack{q,l,i,j \\ i \neq j}} Z_{iq}Z_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}} \right) \\
	& = - \underbrace{\sum_{\substack{q,l,i,j \\ i \neq j}} Z_{iq}Z_{jl}\log (2\lambda_{ql})}_{\sum_{q,l} n_{ql} \log(2\lambda_{ql})} - \underbrace{\sum_{\substack{q,l,i,j \\ i \neq j}} Z_{iq}Z_{jl}\frac{|\Omega_{ij}|}{\lambda_{ql}}}_{\norm{\rho_Z(\Omega)}_1} \\
\end{align*}

Otherwise: 
\[ \log \mathbb{P}(\Omega|Z) = \sum_{i} \left( -\log (2\lambda_{0}) - \frac{|\Omega_{ii}|}{\lambda_{0}} \right) \]


\paragraph{(3):} $Z \sim \mathcal{M}(1,\underline{\alpha})$

\textcolor{red}{Sophie : ici il faut changer tous les $n$ en $p$ = taille de $\Omega$. }
\begin{align*}
\log \mathbb{P}(Z) &= \log \prod_{i = 1}^n \mathbb{P}(Z_i) = \log \prod_{i = 1}^n \prod_{q = 1}^Q \alpha_q^{Z_{iq}} \\
	&=  \sum_{i = 1}^n \sum_{q = 1}^Q \log \alpha_q^{Z_{iq}} =  \sum_{i = 1}^n \sum_{q = 1}^Q Z_{iq} \log \alpha_q \\
\end{align*}

\section{EM algorithm and variational estimation}

\paragraph{Steps}

\begin{enumerate}
	\item Expectation (E): Calculate the expected value of the likelihood under the current parameters. Estimate $Z_i$, knowing $\Omega$ from previous step.
	\item Maximization (M): Find parameters that maximizes the likelihood. Compute parameters knowing $Z_i$.
\end{enumerate}

\subsection{Definitions} 
Let $\mathcal{Z}$ the space of all possibilities. 

Definitions give us: 
\[ \log \mathbb{P}(Z|X, \Omega) = \log \mathbb{P}(X,\Omega, Z) - \log \mathbb{P}(X, \Omega) \]

\subsection{E step: impossibility of direct compuation and variational approach} 
In this part, $\Omega$ is known (as $\Omega^{(t)}$) from a previous M step.

Considering $\mathbb{P}(X, \Omega)$ being constant with respect to $Z$: 

\begin{align*}
\log \mathbb{P}(X, \Omega) &= \log \mathbb{P}(X,\Omega, Z) - \log \mathbb{P}(Z|X, \Omega) \\
	&= \mathbb{E}_{Z|\Omega^(t)}[\log \mathbb{P}(X,\Omega, Z)] - \mathbb{E}_{Z|\Omega^(t)}[\log \mathbb{P}(Z|X, \Omega)] \\
 	&= \sum_{z\in \mathcal{Z}} \mathbb{P}(Z=z | \Omega^{(t)})\log \mathbb{P}(X,\Omega, Z) - \sum_{z\in \mathcal{Z}} \mathbb{P}(Z=z | \Omega^{(t)}) \log \mathbb{P}(Z|X, \Omega) \\ 	 	
 	& = \mathbb{E}_{Z|\Omega^(t)}[\log \mathbb{P}(X,\Omega, Z)] - \mathcal{H}(Z|X) \\
\end{align*}

The expected complete likelihood under the current parameters is:

\[ \mathcal{Q}(\Omega| \Omega^{(t)}) = \mathbb{E}_{Z|\Omega^(t)}[\log \mathbb{P}(X,\Omega, Z)] = \sum_{z\in \mathcal{Z}} \mathbb{P}(Z=z | \Omega^{(t)})\log \mathbb{P}(X,\Omega, Z)\]

$\mathbb{P}(Z_{iq}Z_{jl} = 1 | \Omega_{ij}^{(t)})$ is unknown as $Z_{iq}$ and $Z_{jl}$ are not independant. Variational approach: take an approximation for $\mathbb{P}(Z|\Omega^{t}):= R_t(Z)$ for E step. 

Now we consider the lower bond $\mathcal{J}$ of $\mathbb{P}(X, \Omega)$:
\[ \mathcal{J}_\tau(X, \Omega,  R(Z)) := \log \mathbb{P}(X, \Omega) - D_{KL}\{ R(Z) || \mathbb{P}(Z|\Omega) \}, \]
where $D_{KL}\{ R(Z) || \mathbb{P}(Z|\Omega) \}$ is the Küllback-Leibler divergence (i.e. distance between these two distributions). We take the following distribution for $R(.)$\footnote{cf. Mariadassous and Robin, Uncovering latent structure in valued graphs: a variational approach, Technical Report 10, Statistics for Systems Biology, 2007}
\[ R_\tau (Z) = \prod_{i = 1}^n h_{\underline{\tau_i}}(Z_i), \]
where $h_{\underline{\tau_i}}$ is the density of the multinomial probability distribution $\mathcal{M}(1,\underline{\tau_i})$ and $\underline{\tau_i} = (\tau_{i1}, ..., \tau_{iQ})$ is a random vector containing the parmeters to optimize in the variational approach. Approximation of the probability that vertex $i$ belongs to cluster $q$, $\tau_{iq}$ estimates $\mathbb{P}(Z_{iq} = 1 | \Omega)$, under the constraint $\sum_q \tau_{iq} = 1$ .

Küllback-Leibler divergence:

\begin{align*}
D_{KL}\{ R_\tau(Z) || \mathbb{P}(Z|\Omega) \} &= \sum_{Z \in \mathcal{Z}} R_\tau(Z) \log \frac{R_\tau(Z)}{\mathbb{P}(Z|\Omega)}  \\
&= \sum_{Z \in \mathcal{Z}} R_\tau(Z) \left( \log R_\tau(Z) - \log \mathbb{P}(Z|\Omega) \right) \\
&= -\mathcal{H}(R_\tau (Z)) -  \sum_{Z \in \mathcal{Z}} R_\tau(Z) \log \mathbb{P}(Z|\Omega) \\
\end{align*}

New formula for the bound\footnote{En considérant que $\mathbb{P}(Z|\Omega) = \mathbb{P}(Z|\Omega, X)$ et en remarquant que $\log \mathbb{P}(X,  \Omega)$ est indep de $Z$}: 

\begin{align*}
\mathcal{J}_\tau(X, \Omega,  R(Z)) &= \log \mathbb{P}(X, \Omega) - D_{KL}\{ R(Z) || \mathbb{P}(Z|\Omega) \} \\
&= \log \mathbb{P}(X, \Omega) + \mathcal{H}(R_\tau (Z)) +  \sum_{Z \in \mathcal{Z}} R_\tau(Z) \log \mathbb{P}(Z|\Omega) \\
&= \mathcal{H}(R_\tau (Z)) +  \sum_{Z \in \mathcal{Z}} R_\tau(Z) \left( \log \mathbb{P}(Z|\Omega) +\log \mathbb{P}(X, \Omega) \right)\\
&= \mathcal{H}(R_\tau (Z)) +  \underbrace{\sum_{Z \in \mathcal{Z}} R_\tau(Z) \log L_c(X, \Omega, Z)}_{\hat{Q}_\tau(\Omega) = \mathbb{E}_{\mathcal{R}_\tau}[\log \mathbb{P}(X, \Omega, Z)]} \\
\end{align*}

Objective: maximization of $\mathcal{J}(X, \Omega,  R(Z))$. 

\paragraph{Expression for $\hat{Q}_\tau(\Omega)$} Assume $R_\tau (Z) = \prod_{i = 1}^n h_{\underline{\tau_i}}(Z_i)$

\begin{align*}
\hat{Q}_\tau(\Omega) &= \sum_{Z \in \mathcal{Z}} R_\tau(Z) \log L_c(X, \Omega, Z) = \mathbb{E}_{R_\tau}[\log L_c(X, \Omega, Z)] \\
	&=  \frac{n}{2} \left( \log(|\Omega|) - tr(S\Omega)  - p\log(2\pi) \right) + \sum_{i,j = 1,  j\neq i}^n \sum_{q,l = 1}^Q\tau_{iq}\tau_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}}\right)  +  \sum_{i = 1}^n \sum_{q = 1}^Q \tau_{iq} \log \alpha_q   \\
\end{align*}


\paragraph{Expression for $\mathcal{H}(R_\tau (Z))$} Assume $R_\tau (Z) = \prod_{i = 1}^n h_{\underline{\tau_i}}(Z_i)$

\begin{align*}
\mathcal{H}(R_\tau (Z)) &= -\sum_{i = 1}^n \sum_{q = 1}^Q  \mathbb{P}(Z_{iq} = 1) \log  \mathbb{P}(Z_{iq} = 1) \\
	&= -\sum_{i = 1}^n \sum_{q = 1}^Q  \tau_{iq} \log  \tau_{iq} \\
\end{align*}

\paragraph{Complete expression of $\mathcal{J}_\tau(X, \Omega,  R(Z))$}

\begin{align*}
\mathcal{J}_\tau(X, \Omega,  R(Z)) &= \mathcal{H}(R_\tau (Z)) + \hat{Q}_\tau(\Omega) \\
	&=  \underbrace{\frac{n}{2} \left( \log(|\Omega|) - tr(S\Omega)  - p\log(2\pi) \right)}_{\text{Constant par rapport à Z, et autres, désigné par c dans l'article}} \\
	& + \sum_{\substack{i,j = 1 \\  j\neq i}}^n \sum_{q,l = 1}^Q\tau_{iq}\tau_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}}\right)  +  \sum_{i = 1}^n \sum_{q = 1}^Q \tau_{iq} \log \alpha_q  - \sum_{i = 1}^n \sum_{q = 1}^Q  \tau_{iq} \log  \tau_{iq}\\
\end{align*}

Similar to the one in the complete likelihood expression, the penalty term is: $\sum_{i,j = 1,  j\neq i}^n \sum_{q,l = 1}^Q\tau_{iq}\tau_{jl}\frac{|\Omega_{ij}|}{\lambda_{ql}}$.

\subsection{E step: parameters estimation}

Strategy: estimate $\tau_{iq}$ with fixed $\alpha_{q}$ and $\lambda_{lq}$, then estimate  $\alpha_{q}$ and $\lambda_{lq}$ considering $\hat{\tau}_{iq}$.

\paragraph{Estimating $\hat{\tau}_{iq}$:}

Introducing the constraint using Lagrange multiplier method. Constraint: $\sum_q \tau_{iq} = 1$.

\[ \frac{\partial\mathcal{J}_\tau(X, \Omega, Z) - \lambda(\sum_{q=1}^{Q} \tau_{iq} - 1)}{\partial \tau_{iq}} = \sum_{\substack{j = 1 \\  j\neq i}}^n \sum_{l = 1}^Q \tau_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}}\right)  + \log \alpha_q  - 1 - \log  \tau_{iq} - \lambda \]

\begin{align*}
\tau_{iq} = \exp \left[ \sum_{\substack{j = 1 \\  j\neq i}}^n \sum_{l = 1}^Q \tau_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}}\right)  +  \log \alpha_q  - 1 - \lambda \right] 
	 & = \exp(-1) \exp(-\lambda) \alpha_q \exp \left[  \sum_{\substack{j = 1 \\  j\neq i}}^n \sum_{l = 1}^Q \tau_{jl}\left( -\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}}\right)\right] \\
	= \exp(-1)\exp(-\lambda)\alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \exp \left[\tau_{jl} \left(-\log (2\lambda_{ql}) - \frac{|\Omega_{ij}|}{\lambda_{ql}}\right)\right] 
	&= \exp(-1)\exp(-\lambda)\alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \left( \frac{1}{2\lambda_{ql}} \exp\left(\frac{-|\Omega_{ij}|}{\lambda_{ql}} \right) \right)^{\tau_{jl}} \\
\end{align*}

Using the constraint to find $\lambda$: 

\begin{align*}
\sum_{q=1}^Q \tau_{iq} = 1 &= \exp(-1)\exp(-\lambda)\alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \left( \frac{1}{2\lambda_{ql}} \exp\left(\frac{-|\Omega_{ij}|}{\lambda_{ql}} \right) \right)^{\tau_{jl}} \\
\Rightarrow \exp(\lambda) &= \left[ \sum_{q=1}^Q  \exp(-1)\alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \left( \frac{1}{2\lambda_{ql}} \exp\left(\frac{-|\Omega_{ij}|}{\lambda_{ql}} \right) \right)^{\tau_{jl}}\right]^{-1} \\
\end{align*}

\[\boxed{\hat{\tau}_{iq} = \frac{\exp(-1) \alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \left( \frac{1}{2\lambda_{ql}} \exp\left(\frac{-|\Omega_{ij}|}{\lambda_{ql}} \right) \right)^{\tau_{jl}}}{\sum_{q=1}^Q  \exp(-1)\alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \left( \frac{1}{2\lambda_{ql}} \exp\left(\frac{-|\Omega_{ij}|}{\lambda_{ql}} \right) \right)^{\tau_{jl}}}} \]

In the paper: 

\[\boxed{\hat{\tau}_{iq} \propto \alpha_q \prod_{\substack{j = 1 \\  j\neq i}}^n \prod_{l = 1}^Q \left( \frac{1}{2\lambda_{ql}} \exp\left(\frac{-|\Omega_{ij}|}{\lambda_{ql}} \right) \right)^{\tau_{jl}}} \]


\paragraph{Estimating $\hat{\alpha}_{q}$:} With constraint $\sum_q \alpha_q = 1$.

\[ \left. \frac{\partial\mathcal{J}_\tau(X, \Omega, Z)}{\partial \alpha_q}  \right| _{\tau_{iq}} = \sum_{i=1}^n \sum_{q = 1}^Q \tau_{iq} \frac{1}{\alpha_q}\]
The value if $\alpha_q$ will be derived from the following expression:
\[ \left. \frac{\partial\mathcal{J}_\tau(X, \Omega, Z)- \eta (\sum_q \alpha_q - 1)}{\partial \alpha_q}  \right| _{\tau_{iq}}  = \sum_{i=1}^n \tau_{iq} \frac{1}{\alpha_q} - \eta  \]

\begin{align*}
0 = \sum_{i=1}^n  \tau_{iq} \frac{1}{\alpha_q} - \eta &\Leftrightarrow \alpha_q  =  \frac{\sum_i \tau_{iq}}{\eta} \\
\Leftrightarrow \sum_q \alpha_q  =  \frac{ \sum_q \sum_i \tau_{iq}}{\eta} &\Leftrightarrow 1  =  \frac{n}{\eta} \Leftrightarrow \boxed{\eta  =  n} \\
0 = \sum_{i=1}^n  \tau_{iq} \frac{1}{\alpha_q} - \eta &\Leftrightarrow \alpha_q  =  \frac{\sum_i^n \tau_{iq}}{\eta} \\
\end{align*}

\[ \boxed{\hat{\alpha}_q  =  \frac{\sum_i^n \tau_{iq}}{n}}\]

\paragraph{Estimating $\hat{\lambda}_{lq}$:}

\[ \left. \frac{\partial\mathcal{J}_\tau(X, \Omega, Z)}{\partial \lambda_{ql}}  \right| _{\tau_{iq}} = \sum_{\substack{j = 1 \\  j\neq i}}^p \tau_{iq}\tau_{jl}\left( -\frac{1}{2\lambda_{ql}} + \frac{|\Omega_{ij}|}{\lambda_{ql}^2}\right)\]

\begin{align*}
0 &=  \sum_{\substack{j = 1 \\  j\neq i}}^p\tau_{iq}\tau_{jl}\left( -\frac{1}{2\lambda_{ql}} + \frac{|\Omega_{ij}|}{\lambda_{ql}^2}\right) \\
\Rightarrow 0	& = \sum_{\substack{j = 1 \\  j\neq i}}^p \tau_{iq}\tau_{jl}\left( -\frac{1}{2} + \frac{|\Omega_{ij}|}{\lambda_{ql}}\right) \\
\end{align*}

\[ \boxed{\hat{\lambda}_{ql} = \frac{\sum_{\substack{j = 1 \\  j\neq i}}^n  \tau_{iq}\tau_{jl} |\Omega_{ij}|}{\sum_{\substack{j = 1 \\  j\neq i}}^n \tau_{iq}\tau_{jl}}   }\]




\section{Likelihood penalized point of view}

Maybe talking of "model"  like in Section  \ref{sec:model} is not a good idea. 
We should consider the penalized likelihood approach. 

\paragraph{Inference of graphical model with prior point of view}
Originally,  to estimate $\Omega$, it is standard to consider the lasso penalization
\begin{align*}
 &=   \log \mathbb{P}(X | \Omega) - \lambda^{-1} \sum_{i < j} |\Omega_{ij}| -  \lambda_0^{-1}  \sum_{i=1}^p|\Omega_{ii}| \\
\widehat \Omega_\lambda  &= \arg \max_{\Omega} \mathcal{S}(X,\Omega,\lambda) 
\end{align*}

We see that : 
\begin{align*}
  \lambda^{-1} \sum_{i < j} |\Omega_{ij}| &= \sum_{i < j}  \log \exp\left( - \frac{ \|\Omega\|}{\lambda}\right)\\
&=    \sum_{i < j}  \log\left(\frac{1}{2 \lambda} \exp\left( - \frac{ |\Omega_{ij}| }{\lambda}\right) \right) +   \sum_{i < j} \log\left(2\lambda\right) \\
&=  \log \mathbb{P}(\Omega;\lambda) + Cste(\lambda) + \frac{p(p-1)}{2} \log(\lambda) 
\end{align*}
where $ \mathbb{P}(\Omega;\lambda) $ is the prior such that the $$\Omega_{ij} \sim_{i.i.d} Laplace(0,\lambda), \quad i< j, \mbox{ and } \quad \Omega_{ii} \sim_{i.i.d} Laplace(0,\lambda_0),$$
\emph{normalized over the precision matrices that are positive definite} (meaning that we set a null prior on the non positive definite matrices $\Omega$). More precisely, let $\mathcal{D}$ be the positive definite matrices, we set: 
$$ \mathbb{P}(\Omega;\lambda)  =\frac{\mathbf{1}_{\Omega \in \mathcal{D}}\prod_{i<j} \lambda e^{-\lambda | \Omega_{ij}|} \prod_{i} \lambda_0 e^{-\lambda_0 | \Omega_{ii}|}}{\int \mathbf{1}_{\Omega \in \mathcal{D}}\prod_{i<j} \lambda e^{-\lambda | \Omega_{ij}|} \prod_{i} \lambda_0 e^{-\lambda_0 | \Omega_{ii}|} d\Omega} $$



If $\lambda$ is known, then  the denominator of $\mathbb{P}(\Omega;\lambda)$ does not depend on $\Omega$ and so : 
\begin{align*}
\widehat \Omega_\lambda  &= \arg \max_{\Omega} \mathcal{S}(X,\Omega,\lambda) \\
  &= \arg \max_{\Omega} \;   \log \mathbb{P}(X | \Omega) +  \log \mathbb{P}(\Omega;\lambda)  + Cste(\lambda)\\
&= \arg \max_{\Omega}  \;  \log \mathbb{P}(X | \Omega) + \log   \mathbb{P}(\Omega;\lambda) \\
&=  \arg \max_{\Omega}   \mathbb{P}_\lambda(\Omega | X)
\end{align*}
where $ \mathbb{P}_\lambda(\Omega | X)$ is the posterior distribution corresponding to the prior distribution  $ \mathbb{P}(\Omega;\lambda) $


 




\paragraph{A different prior distribution on $\Omega$}

Now we propose to change the prior distribution : 


We set a SBM prior on $\Omega$. We fix $K$ the number of clusters

\begin{eqnarray*}
 Z_i &\sim& {i.i.d.} \mathcal{M}(1,\pi)  \quad  \forall i \in \{1, \dots, p\} \\
 \Omega_{ij} | Z_i=k,Z_j=\ell &\sim& Laplace(\lambda_{k\ell})  \quad  \forall i<j \in \{1, \dots, p\}\\
 \Omega_{ii} &\sim& Laplace(\lambda_0)  \quad  \forall i  \in \{1, \dots, p\}\\
\end{eqnarray*}
But, to consider the "prior" point of view, we have to restrict this prior  over the definite positive matrices

$$ \mathbb{P}_{\pi,\lambda,K, \mathcal{D}}(\Omega)\propto  \mathbf{1}_{\Omega\in  \mathcal{D}}  \sum_{\mathbf{Z}} P(\Omega |\mathbf{Z}; \lambda) P(\mathbf{Z}; \pi)  = \frac{\mathbf{1}_{\Omega\in  \mathcal{D}}  \sum_{\mathbf{Z}}P(\Omega |\mathbf{Z}; \lambda) P(\mathbf{Z}; \pi) }{ \int{\mathcal{D}}   \sum_{\mathbf{Z}} P(\Omega |\mathbf{Z}; \lambda) P(\mathbf{Z}; \pi)   d\Omega} $$


If $K$,  $\lambda$ and $\pi$ are know then we have to optimize: 
\begin{align*}
\widehat \Omega_{\pi,\lambda,K}  &= \arg \max_{\Omega} \mathcal{S}(X,\Omega, \lambda,\pi,K) \\
\mathcal{S}(X,\Omega, \lambda,\pi,K) &=  \log \mathbb{P}(X | \Omega) +  \log \mathbb{P}_{ \pi,\lambda,K, \mathcal{D} }(\Omega; \theta)    
\end{align*}

However, that way, we have to optimize $\Omega$ under the constraint... 
 


\vspace{1em}



An other strategy similar to the one proposed by \cite{Marlin2009} or \cite{Sun2015} is  to remove the constraint from the prior and  maximise a pseudo-likelihood. More precisely : 

$$
\widehat \Omega_\lambda   = \arg \max_{\Omega} \widetilde{\mathcal{S}}(X,\Omega,\lambda) $$
with 


$$
 \widetilde{\mathcal{S}}(X,\Omega,\lambda,\pi,K)  =  \log \widetilde{\mathcal{S}}(X | \Omega)  +   \log \mathbb{P}_{\pi,\lambda,K}(\Omega) $$
where 
$$ \mathbb{P}_{\pi,\lambda,K}(\Omega) =   \sum_{\mathbf{Z}} P(\Omega |\mathbf{Z}; \lambda) P(\mathbf{Z}; \pi)$$
and 
$\log \widetilde{\mathcal{S}}(X | \Omega) $ is the linear regression version of the $\Omega_{ij}$. 



\vspace{1em} 

\paragraph{\textcolor{red}{New strategy}}
An other strategy could be to put a SBM prior distribution on the $\Omega_{ij}$ ($i<j$) and set $\Omega_{ii} = \sum_{j \neq i} \Omega_{ij}$
In that case, the matrix $\Omega$ automatically becomes  positive definite.  It could be any other transformation making the matrix inversible. 


So let us define : $ \Omega_T$ the upper triangular coefficients of $\Omega$. We set a prior SBM distribution on $\Omega_T$. 
$\Omega$ is deduced deterministically from $\Omega_T$.  We denote by $\Phi$ this deterministic function. 
 

We set : 
$$
 \widetilde{\widetilde{\mathcal{S}}}(X,\Omega_T,\lambda,\Omega,\lambda,\pi,K)  =  \log  \mathbb{P}(X | \Phi(\Omega_T))  +   \log \mathbb{P}_{\pi,\lambda,K}(\Omega_T) $$

We want to optimize it with respect to $\Omega_T$ and $\pi,\lambda$ (for a fixed $K$). We won't be able to do it because $  \log \mathbb{P}_{\pi,\lambda,K}(\Omega_T)$ has no explicit expressions. We introduced a lower bound of $\widetilde{\widetilde{\mathcal{S}}}(X,\Omega_T,\lambda,\Omega,\lambda,\pi,K)$, namely $\mathcal{J}(X,\Omega_T,\lambda,\Omega,\lambda,\pi,K,\tau)$

\begin{eqnarray*}
\mathcal{J}_K(X;\Omega_T,\lambda,\pi,\tau) &=& \log  \mathbb{P}(X | \Phi(\Omega_T))  + \log \mathbb{P}_{\pi,\lambda,K}(\Omega_T)  -  \mathcal{K}(\mathcal{R}_{\tau}, P(Z | \Omega))\\
 &=&  \log  \mathbb{P}(X | \Phi(\Omega_T))  + \mathbb{E}_{\mathcal{R}_{\tau}} \left[\log  P(\Omega_T ,\mathbf{Z}; \lambda,\pi)    \right] + \mathcal{H}(\mathcal{R}_{\tau}(\cdot))
 \end{eqnarray*}


We can optimize it iteratively. At iteration $t$. 

\begin{enumerate}
\item  $$(\pi^{(t)},\lambda^{(t)}) = \arg \max_{\pi,\lambda} \mathcal{J}_K(X;\Omega_T^{(t-1)},\lambda,\pi,\tau^{(t-1)})$$
It is the M-step of the VEM. 
\item   $$\tau^{(t)} = \arg \max_{\tau}  \mathcal{J}_K(X;\Omega_T^{(t-1)},\lambda^{(t)},\pi^{(t)},\tau)$$

It the $E$-step of the VEM. Resulting into a fixed point equation.  We could also use a lower bound of this preventing from resolving a fixed point equation (GEM). 

\item 
\begin{eqnarray*}
\Omega^{(t)}_T &=& \arg \max_{\Omega_T}  \log  \mathbb{P}(X | \Phi(\Omega_T))  + \mathbb{E}_{\mathcal{R}_{\tau^{(t)}}} \left[\log  P(\Omega_T ,\mathbf{Z}; \lambda^{(t)},\pi^{(t)})    \right] + \mathcal{H}(\mathcal{R}_{\tau^{(t)}}(\cdot))\\
&=&  \arg \max_{\Omega_T}   \left[ \log  \mathbb{P}(X | \Phi(\Omega_T))  +  \sum_{i<j} \sum_{k,l}\tau^{(t)}_{ik}\tau^{(t)}_{j\ell} (  \log \lambda^{(t)}_{k,\ell}  - \lambda^{(t)}_{k\ell} |\Omega_{ij}| )  \right]  \\
&=&  \arg \max_{\Omega_T}    \left[ \log  \mathbb{P}(X | \Phi(\Omega_T))  -   \sum_{i<j} \sum_{k,l}\tau^{(t)}_{ik}\tau^{(t)}_{j\ell}   \lambda^{(t)}_{k\ell} |\Omega_{ij}|   \right]    
\end{eqnarray*}
\textcolor{red}{I state that it is equivalent to estimating  $\Omega$ in its complete version (upper and diagonal) under the constraint  $\Omega_{ii} = \sum_{i\neq j } \Omega_{ij}$ which is a linear constraint : 
$$  \Omega_{ii} -  \sum_{i\neq j } \Omega_{ij}=0$$}
Using a Lagrange approach, we get: 
\begin{eqnarray*}
\Omega^{(t)}  &=& \arg \max_{\Omega}    \left[ \log  \mathbb{P}(X | \Omega )  -   \sum_{i<j} \sum_{k,l}\tau^{(t)}_{ik}\tau^{(t)}_{j\ell}   \lambda^{(t)}_{k\ell} |\Omega_{ij}|   + \sum_{i=1}^p\nu_i    \left( \Omega_{ii} -  \sum_{i\neq j } \Omega_{ij} \right) \right]
  \end{eqnarray*}
And $\Omega_T^{(t)}$ is the upper triangular part of $\Omega^{(t)}$
\textcolor{red}{Is it feasible Julien ???? }




\end{enumerate}

\textcolor{green}{Extension}
On peut aussi juste mettre la contrainte  $\Omega_{ii} > \sum_{i\neq j } \Omega_{ij} $ ce qui pourrait être fait en posant 
$$\Omega_{ii}  = \sum_{i\neq j } \Omega_{ij}  + \mu_i  \quad \mbox{with} \quad \mu_i \sim_{i.i.d.}  Laplace(\lambda_0)$$ 


\textcolor{green}{Question}

Est ce que cela a  un sens de contraindre $\Omega$ dans cet espace?  Est ce qu'on perd beaucoup de matrices $\Omega$?  

\section{A new approach : a block model structure on the Cholesky decomposition of $\Omega$}

Let $L$ be a lower triangular matrix such that $L_{ii}>0$ for any $i$. 
Then $\Omega  = L L^T$ is a symatric definite positive matrix. 
As a consequence, let us propose a SBM prior distribution on $L$. 


We set the following model prior distribution on  $L$. We fix $K$ the number of clusters

\begin{eqnarray*}
 Z_i &\sim& {i.i.d.} \mathcal{M}(1,\pi)  \quad  \forall i \in \{1, \dots, p\} \\
 L_{ij} | Z_i=k,Z_j=\ell &\sim&_{i.i.d} Laplace(\lambda_{k\ell})  \quad  \forall i<j \in \{1, \dots, p\}\\
 L_{ii} &\sim& \Gamma(\alpha_0, \beta_0)  \quad  \forall i  \in \{1, \dots, p\}\\
\Omega &=& L L^T
\end{eqnarray*}
A so
$$\Omega_{ij} = \sum_{r=1}^j L_{ir} L_{jr}, \forall i \geq j$$

\paragraph{Do we get from that a block structure on $\Omega$}

A a consequence, $\forall i >j$ 
\begin{eqnarray*}
\mathbb{E}[\Omega_{ij} | Z_i = k, Z_j = \ell] &=& \mathbb{E}\left[\sum_{r=1}^j L_{ir} L_{jr} | Z_i = k, Z_j = \ell\right]\\
&=&  \sum_{r=1}^{j-1}  \mathbb{E} \left[ L_{ir} L_{jr} | Z_i = k, Z_j = \ell\right] +  \mathbb{E} \left[ L_{ij} L_{jj} | Z_i = k, Z_j = \ell\right]  \\
&=& \sum_{r=1}^{j-1}   \sum_{v = 1}^K  \mathbb{E}\left[ L_{ir} L_{jr} | Z_i = k, Z_j = \ell, Z_r = v\right] P(Z_r = v) +  \mathbb{E} \left[ L_{ij} | Z_i = k, Z_j = \ell\right]     \mathbb{E} \left[ L_{jj} | Z_i = k, Z_j = \ell\right]\\
&=& \sum_{r=1}^{j-1}  \sum_{v = 1}^K   \mathbb{E}\left[ L_{ir} | Z_i = k, Z_r = v\right]    \mathbb{E}\left[L_{jr} |  Z_j = \ell, Z_r = v\right] \pi_v + \mu_{k \ell} \cdot  \mu_0\\
&=&  \sum_{r=1}^{j-1} \sum_{v = 1}^K \mu_{kv} \mu_{\ell v} \pi_v + \mu_{k \ell} \cdot  \mathbb{E}[L_{jj}]\\
&=& (j-1)   \sum_{v = 1}^K  \mu_{kv} \mu_{\ell v} \pi_v  +    \mu_{k \ell} \cdot   \mathbb{E}[L_{jj}]\\
&=& F(j,k,\ell)
\end{eqnarray*}


\begin{center}
\fbox{No we don't!!!!! $\rightarrow$ espoirs douchés} 

If $\mathbb{E}[L_{jj}]  = (j-1)  \mu_0$, then  : 
$$
\mathbb{E}[\Omega_{ij} | Z_i = k, Z_j = \ell] = (j-1)  \Phi(k,\ell)
$$
so  we have a block structure on $\frac{\Omega_{ij}}{j-1}$. Not sure it is useful... 
\end{center}


\bibliographystyle{plain}
\bibliography{biblio}
  
 \end{document}